{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/ECU/Library/Python/3.9/lib/python/site-packages/tqdm/auto.py:22: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n",
      "Generate config GenerationConfig {\n",
      "  \"bos_token_id\": 50256,\n",
      "  \"eos_token_id\": 50256,\n",
      "  \"transformers_version\": \"4.26.1\"\n",
      "}\n",
      "\n",
      "loading configuration file generation_config.json from cache at aitextgen/models--gpt2/snapshots/e7da7f221d5bf496a48136c0cd264e630fe9fcc8/generation_config.json\n",
      "Generate config GenerationConfig {\n",
      "  \"_from_model_config\": true,\n",
      "  \"bos_token_id\": 50256,\n",
      "  \"eos_token_id\": 50256,\n",
      "  \"transformers_version\": \"4.26.1\"\n",
      "}\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from aitextgen import aitextgen\n",
    "\n",
    "ai = aitextgen()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Generate config GenerationConfig {\n",
      "  \"bos_token_id\": 50256,\n",
      "  \"eos_token_id\": 50256,\n",
      "  \"transformers_version\": \"4.26.1\"\n",
      "}\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1mWhat is sentience\u001b[0m, where does it come from? Why do we have to know about this world because we don't know it?\" We are not told that we are supposed to understand it, or that we are supposed to know it, or that we are supposed to use it to help our parents, or that we are supposed to be able to see it in its true light. We are not told that it is the world we live in, or that we are supposed to know about it. We are told that it is something we do in our everyday lives, and that we are supposed to have it and to use it to help us.\n",
      "\n",
      "When we think of our children, we think about our parents, as if that were something really special. For many years now, I've seen parents who have never met a child in their lives, who have never met their children, who have never met their children. We've seen a mother who has never met her children, who has never met her children, who has never met her children, who has never met her children. I've seen a mother who has never met her children, who has never met her children, who has never met her children. But we do not have to \"go to the grocery\n",
      "==========\n",
      "\u001b[1mWhat is sentience\u001b[0m and what is meant by it?\n",
      "\n",
      "It is a philosophical question. It is not a question of what is sentience or what is meant by it. What is sentience means what is sentience? What is meant by it is what is sentience? There are two ways we might answer this question. The first way is to say that the soul is sentience. We can say that the soul is sentience, because we know that it is sentience. The second way is to say that the soul is sentience because it is sentience. We can say that the soul is sentience because it is sentience. We can say that the soul is sentience because it is sentience.\n",
      "\n",
      "But what is meant by it?\n",
      "\n",
      "It is the ability to create something, to create something. One of the reasons that God did not create a human is that he did not create a human. He created God. And the only thing that they did create was human nature. For God created the world, because he did not create Adam and Eve. He created the world because he did not create Adam and Eve. He created the world because he did not create Adam and Eve. He created the world because he did not create\n",
      "==========\n",
      "\u001b[1mWhat is sentience\u001b[0m?\n",
      "\n",
      "When we're able to communicate with each other we can say \"Hello\" to each other and \"Don't worry about me.\" When we're able to communicate with each other we can say \"Hello\" to each other and \"Don't worry about me.\"\n",
      "\n",
      "What is sentience?\n",
      "\n",
      "When we're able to communicate with each other we can say \"Hello\" to each other and \"Don't worry about me.\" When we're able to communicate with each other we can say \"Hello\" to each other and \"Don't worry about me.\"\n",
      "\n",
      "What is sentience?\n",
      "\n",
      "When we're able to communicate with each other we can say \"Hello\" to each other and \"Don't worry about me.\" When we're able to communicate with each other we can say \"Hello\" to each other and \"Don't worry about me.\"\n",
      "\n",
      "What is sentience?\n",
      "\n",
      "When we're able to communicate with each other we can say \"Hello\" to each other and \"Don't worry about me.\" When we're able to communicate with each other we can say \"Hello\" to each other and \"Don't worry about me.\n",
      "\n",
      "What is sentience?\n",
      "\n",
      "When we're able to\n",
      "None\n"
     ]
    }
   ],
   "source": [
    "print(ai.generate(prompt=\"What is sentience\" , n=3))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Generate config GenerationConfig {\n",
      "  \"bos_token_id\": 50256,\n",
      "  \"eos_token_id\": 50256,\n",
      "  \"transformers_version\": \"4.26.1\"\n",
      "}\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1mare u a A.I\u001b[0m.A.M.E.A.R.D.A.R.E.A.R.E.\n",
      "\n",
      "Please enter a valid IBAN number.\n",
      "\n",
      "You need to accept to charge your bank account.\n",
      "\n",
      "Your billing zip code needs to be 5 digits.\n",
      "\n",
      "Please double check your CEP info. The CEP format should be something like 12345-678.\n",
      "\n",
      "Please double check your tax identifier.\n",
      "\n",
      "There was a problem saving your address.\n",
      "\n",
      "There was a problem saving your card info.\n",
      "\n",
      "There was a problem saving your personal information.\n",
      "\n",
      "McAfee Secure sites help keep you safe from identity theft, card fraud, spyware, spam, viruses and online scams.\n",
      "\n",
      "Copying Prohibited by Law - McAfee Secure is a Trademark of McAfee, Inc.\n",
      "\n",
      "Unknown card type.\n",
      "\n",
      "No card number provided.\n",
      "\n",
      "card number is in invalid format.\n",
      "\n",
      "Wrong card type or card number is invalid.\n",
      "\n",
      "card number has an inappropriate number of digits.\n",
      "\n",
      "Please enter numbers here.\n",
      "\n",
      "Please enter an integer value.\n",
      "\n",
      "Numbers must be less or equal to $$$$\n",
      "\n",
      "All the required fields\n",
      "==========\n",
      "\u001b[1mare u a A.I\u001b[0m.O.S.S.A.S.A.C. (A.N.A.C.A.S.A) (A.N.A.C.A.S.A.C.) (A.N.A.C.A.S.A.) (A.N.A.C.A.S.A.) (A.N.A.C.A.S.) (A.N.A.C.A.S.A.) (A.N.A.C.A.S.) (A.N.A.C.A.S.) (A.N.A.C.A.S.) (A.N.A.C.A.S.) (A.N.A.C.A.S.A.) (A.N.A.C.A.S.) (A.N.A.C.A.S.) (A.N.A.C.A.S.) (A.N.A.C.A.S.A.) (A.N.A.C.A.S.) (A.N.A.C.A.\n",
      "==========\n",
      "\u001b[1mare u a A.I\u001b[0m. A.I. a.a.t.f.a.i.n.n.o.d.c.e.c.e.d.e.d.e.d.e.d.e.d.e.d.e.d.e.d.e.d.e.d.e.d.e.d.e.d.e.d.e.d.e.d.e.d.e.d.e.d.e.d.e.e.d.e.d.e.d.e.d.e.d.e.d.e.d.e.d.e.d.e.d.e.d.e.d.e.d.e.d.e.d.e.d.e.d.e.d.e.d.e.d.e.d.e.d.e.d.e.d.e.d.e.d.e.d.e.d.e.d.e.d.e.d.e.d.e.d.e.d.e\n",
      "None\n"
     ]
    }
   ],
   "source": [
    "print(ai.generate(prompt=\"are u a A.I\", n=3))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "from aitextgen.TokenDataset import TokenDataset\n",
    "from aitextgen.tokenizers import train_tokenizer\n",
    "from aitextgen.utils import GPT2ConfigCPU\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "train_tokenizer('Shakespeare.txt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Generate config GenerationConfig {\n",
      "  \"bos_token_id\": 0,\n",
      "  \"eos_token_id\": 0,\n",
      "  \"transformers_version\": \"4.26.1\"\n",
      "}\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 40000/40000 [00:00<00:00, 88082.46it/s] \n"
     ]
    }
   ],
   "source": [
    "config = GPT2ConfigCPU()\n",
    "\n",
    "ai = aitextgen(tokenizer_file=\"aitextgen.tokenizer.json\", config=config)\n",
    "data = TokenDataset('Shakespeare.txt' , tokenizer_file='aitextgen.tokenizer.json', block_size=64)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "pytorch_model.bin already exists in /trained_model and will be overwritten!\n",
      "GPU available: True (mps), used: False\n",
      "TPU available: False, using: 0 TPU cores\n",
      "IPU available: False, using: 0 IPUs\n",
      "HPU available: False, using: 0 HPUs\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "                                                                              \n",
      "\u001b[A                                                                          \u001b[1m5,000 steps reached: saving model to /trained_model\u001b[0m\n",
      "Loss: 3.340 — Avg: 3.301:  17%|█▋        | 8280/50000 [10:26<52:36, 13.22it/s]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Configuration saved in trained_model/generation_config.json\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                                                                              \n",
      "\u001b[A                                                                          \u001b[1m5,000 steps reached: generating sample texts.\u001b[0m\n",
      "Loss: 3.340 — Avg: 3.301:  17%|█▋        | 8280/50000 [10:26<52:36, 13.22it/s]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Generate config GenerationConfig {\n",
      "  \"bos_token_id\": 0,\n",
      "  \"eos_token_id\": 0,\n",
      "  \"transformers_version\": \"4.26.1\"\n",
      "}\n",
      "\n",
      "A decoder-only architecture is being used, but right-padding was detected! For correct generation results, please set `padding_side='left'` when initializing the tokenizer.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                                                                              \n",
      "\u001b[A                                                                          ==========\n",
      "                                                                              \n",
      "\u001b[A                                                                          .\n",
      "\n",
      "ROMEO:\n",
      "Death cousin, sir, welcome, for my thrive,\n",
      "And getings sorrow's mortal.\n",
      "\n",
      "SICINIUS:\n",
      "O,\n",
      "Old save her.\n",
      "\n",
      "AUTOLYCUS:\n",
      "\n",
      "                                                                              \n",
      "\u001b[A                                                                          ==========\n",
      "Loss: 3.340 — Avg: 3.301:  17%|█▋        | 8280/50000 [10:26<52:36, 13.22it/s]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "`Trainer.fit` stopped: `max_steps=5000` reached.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss: 3.140 — Avg: 3.151: 100%|██████████| 5000/5000 [03:56<00:00, 21.11it/s]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Configuration saved in trained_model/generation_config.json\n"
     ]
    }
   ],
   "source": [
    "ai.train(data, batch_size=8, num_steps=5000, generate_every=5000, save_every=5000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Generate config GenerationConfig {\n",
      "  \"bos_token_id\": 0,\n",
      "  \"eos_token_id\": 0,\n",
      "  \"transformers_version\": \"4.26.1\"\n",
      "}\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1mROMEO\u001b[0m:\n",
      "\n",
      "JULIET:\n",
      "Dispatch; I, till I do, and go to me\n",
      "And now I'll not fear to die on to yourself.\n",
      "\n",
      "RACLIS:\n",
      "O, fie, good lords, behold my lord.\n",
      "==========\n",
      "\u001b[1mROMEO\u001b[0m:\n",
      "I would have tooken till hence.\n",
      "\n",
      "ROMEO:\n",
      "A prison, and so, I will not say the woe.\n",
      "\n",
      "Nurse:\n",
      "And so is't laws;\n",
      "I'll not, forsworn me, if I wake,\n",
      "==========\n",
      "\u001b[1mROMEO\u001b[0m:\n",
      "He would not mortalk, and his life.\n",
      "\n",
      "KING RICHARD II:\n",
      "My lords, I will not become to go;\n",
      "And therefore thou didst, my liege, a villains,\n",
      "And thou art not for my king;\n",
      "And I\n",
      "==========\n",
      "\u001b[1mROMEO\u001b[0m:\n",
      "Not a word, for love, which is sister.\n",
      "\n",
      "ANTONIO:\n",
      "Yet, I pray you, sir; I am, this\n",
      "To call you mercy: you, sir, your lord,\n",
      "good merry-groan,\n",
      "==========\n",
      "\u001b[1mROMEO\u001b[0m:\n",
      "\n",
      "DUKE OF YORK:\n",
      "I hope I'll believe thee to thee,\n",
      "And I will not for love, my lord, I'll not die.\n",
      "\n",
      "QUEEN ELIZABhamberland;\n",
      "Beholdly, forthine ownness within.\n",
      "==========\n",
      "\u001b[1mROMEO\u001b[0m:\n",
      "But where is my lords, in love, and art thou,\n",
      "They may be obeous, that hath borne\n",
      "To your queen and all the fruit of life.\n",
      "\n",
      "KING WorILI:\n",
      "To meet thee to him. Ifort\n",
      "==========\n",
      "\u001b[1mROMEO\u001b[0m:\n",
      "A last of your grace's voices are fellow,\n",
      "A husbandess.\n",
      "\n",
      "MIRANDA:\n",
      "No; I have no man; he will be\n",
      "In this sinter to window so.\n",
      "\n",
      "POMPE\n",
      "==========\n",
      "\u001b[1mROMEO\u001b[0m:\n",
      "Say, thou wilt he not be moon,\n",
      "Who shall try against the daughter of purpose.\n",
      "What, I'll take him off:\n",
      "To say your traitors, and that I do\n",
      "Thus fellow of this n\n",
      "==========\n",
      "\u001b[1mROMEO\u001b[0m: I said, sir, as to deter\n",
      "thereian to a virty.\n",
      "\n",
      "BENVOLIO:\n",
      "I was fond as, my lord to do a ped.\n",
      "\n",
      "BIONDELLO:\n",
      "I have too: what he's\n",
      "==========\n",
      "\u001b[1mROMEO\u001b[0m:\n",
      "The subst is all the seel.\n",
      "\n",
      "KING RICHARD III:\n",
      "O, a thousand turn'd, and even now.\n",
      "\n",
      "DUCHESS OF YORK:\n",
      "Has devilin, glority, thou based,\n",
      "To\n"
     ]
    }
   ],
   "source": [
    "ai.generate(10, prompt=\"ROMEO\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.6"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "31f2aee4e71d21fbe5cf8b01ff0e069b9275f58929596ceb00d14d90e3e16cd6"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
